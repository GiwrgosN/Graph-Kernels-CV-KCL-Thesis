{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ebc89b",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa13ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports all the libraries needed \n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "from skimage import data, segmentation, color\n",
    "from skimage import graph\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from skimage.segmentation import slic\n",
    "from skimage.color import rgb2lab\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from grakel.kernels import ShortestPath\n",
    "from grakel import Graph\n",
    "import scipy.io\n",
    "import os\n",
    "import glob\n",
    "from grakel import GraphKernel\n",
    "from grakel.kernels import ShortestPathAttr, SubgraphMatching, PropagationAttr\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import cv2\n",
    "import pickle\n",
    "from grakel.kernels import WeisfeilerLehman, VertexHistogram, NeighborhoodHash, WeisfeilerLehmanOptimalAssignment, EdgeHistogram\n",
    "import csv\n",
    "import statistics\n",
    "from skimage import io, color\n",
    "import math\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "num_images = 200\n",
    "\n",
    "# Set the random seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcd33a9",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f832461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_image(image: np.array) -> np.array:\n",
    "    \"\"\" Downsample every image in order to speed up calculations.\n",
    "        We choose every second pixel -> Reduce size in half. \n",
    "    :param image: The original image of type `np.array`.\n",
    "    :return: Return the same image downsampled half the size. \n",
    "    \"\"\"\n",
    "    return image[0::2, 0::2]\n",
    "\n",
    "\n",
    "def load_image(path: str)->np.array:\n",
    "    \"\"\"\n",
    "     Load the image from the specified path. \n",
    "     Pixel intensities take values in the range [0,255] and are represented using all 3 channels (RGB). \n",
    "     redChannel   = image_imported[:,:,0] - Red channel\n",
    "     greenChannel = image_imported[:,:,1] - Green channel\n",
    "     blueChannel  = image_imported[:,:,2] - Blue channel\n",
    "    \n",
    "    :param path: The path of the image of type `str`.\n",
    "    :return image: The image to return stored as `np.array`.\n",
    "    \"\"\"\n",
    "    image_imported = Image.open(path)\n",
    "    image = np.array(image_imported)\n",
    "    image = downsample_image(image)\n",
    "    return image\n",
    "\n",
    "\n",
    "def load_ground_truth_images()-> dict:\n",
    "    \"\"\" Create a dictionary with groundtruth segmentations for each image in the training set.\n",
    "    \n",
    "    :return groundtruth: Dictionary of the form key:[segmentation,number of segments,chosen_truth]. For each image\n",
    "    we might have more than 1 ground truth segmentations so we will return the segmentation with the MINIMUM \n",
    "    number of segments. Key is going to be the number of the image. \n",
    "    \"\"\"\n",
    "    folder_path = \"C:/Users/giwrg/Desktop/Master/Modules/Thesis/Reading Material & Data/2- Data/Data Used - 500/groundTruth/train\"  \n",
    "    # Construct the pattern to match image files\n",
    "    image_pattern = os.path.join(folder_path, '*.mat')  \n",
    "    # Use glob to find all matching image file paths\n",
    "    image_paths = glob.glob(image_pattern)\n",
    "    train_paths = []\n",
    "    groundtruth = {}\n",
    "    # Replace \\ with /\n",
    "    for path in image_paths:\n",
    "        updated_string = path.rsplit(\"\\\\\", 1)\n",
    "        updated_string = \"/\".join(updated_string)\n",
    "        train_paths.append(updated_string)\n",
    "\n",
    "    for path in image_paths:\n",
    "        mat_path = path\n",
    "        # Get the file name from the path\n",
    "        file_name = os.path.basename(path)\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        # Extract the number\n",
    "        image_number = file_name.split(\"\\\\\")[-1]\n",
    "\n",
    "        mat_contents = scipy.io.loadmat(mat_path)\n",
    "        num_of_groundtruths = mat_contents['groundTruth'][0].shape[0]\n",
    "        num_regions = []\n",
    "        for i in range(0,num_of_groundtruths):\n",
    "            # Find the number of distinct elements\n",
    "            num_distinct_elements = len(np.unique(mat_contents['groundTruth'][0][i][0][0][0]))\n",
    "            num_regions.append(num_distinct_elements)\n",
    "        max_index = num_regions.index(min(num_regions))  # max_index = Index of ground truth with the minimum number of segments\n",
    "        selected_truth = mat_contents['groundTruth'][0][max_index][0][0][0]\n",
    "        # Downsample true segmentation \n",
    "        selected_truth = downsample_image(selected_truth)\n",
    "        groundtruth[image_number] = [selected_truth,len(np.unique(selected_truth)), max_index]\n",
    "    return groundtruth\n",
    "    \n",
    "\n",
    "def superpixel_SLIC(image: np.array, n_segments: int, compactness: int) ->np.array:\n",
    "    \"\"\"\n",
    "    Create superpixels using SLIC algorithm.\n",
    "    \n",
    "    \n",
    "    :param image: Image of type 'np.array' having all 3 channels (RGB format)\n",
    "    :param n_segments: Approximate number of superpixels of type `int`.\n",
    "    :param compactness: Defines the tradeoff between space and color proximity of type `int`. \n",
    "     High m  -> more square superpixels.\n",
    "     Small m -> arbitary shapes for superpixels but more sensitive to boundries.\n",
    "    :return superpixels: The superpixel regions of type 'np.array'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use lab color format\n",
    "    lab = rgb2lab(image)\n",
    "\n",
    "    # Use SLIC algorithm for superpixel segmentation\n",
    "    superpixels = slic(lab, n_segments=n_segments, compactness=compactness, start_label=1)\n",
    "    return superpixels\n",
    "\n",
    "\n",
    "def Euclidean_distance(vector_a:list, vector_b:list)->float:\n",
    "    \"\"\" Calculate Euclidean distance between co-ordinates-vectors of 2 pixels. \n",
    "    \n",
    "    :param vector_a: Co-ordinates of first node of type `list`.\n",
    "    :param vector_b: Co-ordinates of second node of type `list`.\n",
    "    :return result: Return the Euclidean distance of co-ordinates of type `int`. \n",
    "    \"\"\"\n",
    "    result = np.sqrt((vector_a[0]-vector_b[0])**2 + (vector_a[1]-vector_b[1])**2)\n",
    "    return result\n",
    "\n",
    "\n",
    "def rgb_to_grayscale(rgb):\n",
    "    \"\"\" Compute grayscale value from RGB values. \n",
    "    :param rgb: The RGB value of type `list`.\n",
    "    :return grayscale: The pixel intensity grayscale value in the range [0,255] of type `int`.\n",
    "    \"\"\"\n",
    "    grayscale = int(0.2989 * rgb[0] + 0.5870 * rgb[1] + 0.1140 * rgb[2])\n",
    "    return grayscale\n",
    "\n",
    "\n",
    "def edge_labels_calc(vector_a:list, vector_b:list)->str:\n",
    "    \"\"\" Calculate edge labels by concatinating pixel intensity values. \n",
    "    \n",
    "    :param vector_a: RGB values of first node of type `list`.\n",
    "    :param vector_b: RGB values of second node of type `list`.\n",
    "    :return: String with edge label.     \n",
    "    \"\"\"\n",
    "    # Convert to grayscale and concatinate. \n",
    "    A = rgb_to_grayscale(vector_a)\n",
    "    B = rgb_to_grayscale(vector_b)\n",
    "    return str(A)+str(B)\n",
    "\n",
    "\n",
    "def get_image_number(path:str)->str:\n",
    "    \"\"\" Extract image number from path. \n",
    "    :param path: The path of the image of type `str`.\n",
    "    :return image_number: Return the image number of type `str`.\n",
    "    \"\"\"\n",
    "    # Get the file name from the path\n",
    "    file_name = os.path.basename(path)\n",
    "    # Remove the file extension\n",
    "    file_name = os.path.splitext(file_name)[0]\n",
    "    # Extract the number\n",
    "    image_number = file_name.split(\"\\\\\")[-1]\n",
    "    return image_number\n",
    "\n",
    "\n",
    "def create_graphs(image: np.array, superpixels: np.array)->list:\n",
    "    \"\"\"\n",
    "    Create a list of Graphs where each graph corresponds to a superpixel. Every node in each graph has\n",
    "    as attributes the RGB values. Each node is connected with at-most 8 neighboors. \n",
    "    \n",
    "    :param image: Image of type 'np.array' having all 3 channels (RGB format).\n",
    "    :param superpixels: The superpixel regions of type 'np.array'.\n",
    "    :return image_graphs: A `list` with graphs for each superpixel. Each graph is an instance of the class\n",
    "    `Graph` within the Grakel library.  \n",
    "    \"\"\"\n",
    "    num_of_superpixels = len(np.unique(superpixels)) # The number of superpixels-graphs \n",
    "    image_graphs = []\n",
    "    for graph_num in range(1,num_of_superpixels+1):\n",
    "        indices = np.argwhere(superpixels == graph_num)  # Indices of the pixels which are in the specific superpixel\n",
    "        adj_matrix = np.zeros((len(indices), len(indices)))  # Adjacency matrix\n",
    "        node_attributes = {}  # Dictionary with attributes for each vertex.\n",
    "        examined_vertices = []\n",
    "        vertex_index = 0\n",
    "        for vector in indices:\n",
    "            x_coordinate       = vector[0]\n",
    "            y_coordinate       = vector[1]\n",
    "            redChannel_value   = image[x_coordinate,y_coordinate,0] # Red channel pixel value \n",
    "            greenChannel_value = image[x_coordinate,y_coordinate,1] # Green channel pixel value\n",
    "            blueChannel_value  = image[x_coordinate,y_coordinate,2] # Blue channel pixel value\n",
    "                        \n",
    "            # Update dictionary with attributes for each vertex\n",
    "            node_attributes[vertex_index] = [redChannel_value, greenChannel_value, blueChannel_value]\n",
    "            if examined_vertices == []:  # This is the first vertex to be added\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "            else:\n",
    "                # Update adjacency matrix\n",
    "                for index, existing_vertex in enumerate(examined_vertices):\n",
    "                    if Euclidean_distance(existing_vertex, vector) < 1.45:  # Use a 8-neighbourhood\n",
    "                        adj_matrix[index, vertex_index] = 1\n",
    "                        adj_matrix[vertex_index, index] = 1\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "        G = Graph(adj_matrix, node_labels=node_attributes)\n",
    "        image_graphs.append(G)\n",
    "    return image_graphs\n",
    "\n",
    "\n",
    "def create_graphs_grayscale(image: np.array, superpixels: np.array)->list:\n",
    "    \"\"\"\n",
    "    Create a list of Graphs where each graph corresponds to a superpixel. Every node in each graph has\n",
    "    as label the grayscale pixel intensity value in the range [0,255]. Each node is connected with at-most 8 neighboors. \n",
    "    \n",
    "    :param image: Image of type 'np.array' having all 3 channels (RGB format).\n",
    "    :param superpixels: The superpixel regions of type 'np.array'.\n",
    "    :return image_graphs: A `list` with graphs for each superpixel. Each graph is an instance of the class\n",
    "    `Graph` within the Grakel library. \n",
    "    \"\"\"\n",
    "    num_of_superpixels = len(np.unique(superpixels)) # The number of superpixels-graphs \n",
    "    image_graphs = []\n",
    "    for graph_num in range(1,num_of_superpixels+1):\n",
    "        indices = np.argwhere(superpixels == graph_num)  # Indices of the pixels which are in the specific superpixel\n",
    "        adj_matrix = np.zeros((len(indices), len(indices)))  # Adjacency matrix\n",
    "        node_labels = {}  # Dictionary with labels for each vertex.\n",
    "        examined_vertices = []\n",
    "        vertex_index = 0\n",
    "        for vector in indices:\n",
    "            x_coordinate       = vector[0]\n",
    "            y_coordinate       = vector[1]\n",
    "            image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "            pixel_intensity    = image_gray[x_coordinate,y_coordinate]\n",
    "            # Update dictionary with attributes for each vertex\n",
    "            node_labels[vertex_index] = pixel_intensity\n",
    "            if examined_vertices == []:  # This is the first vertex to be added\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "            else:\n",
    "                # Update adjacency matrix\n",
    "                for index, existing_vertex in enumerate(examined_vertices):\n",
    "                    if Euclidean_distance(existing_vertex, vector) < 1.45:  # Use a 8-neighbourhood\n",
    "                        adj_matrix[index, vertex_index] = 1\n",
    "                        adj_matrix[vertex_index, index] = 1\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "        G = Graph(adj_matrix, node_labels=node_labels)\n",
    "        image_graphs.append(G)\n",
    "    return image_graphs\n",
    "\n",
    "\n",
    "def create_graphs_edges(image: np.array, superpixels: np.array)->list:\n",
    "    \"\"\"\n",
    "    Create a list of Graphs where each graph corresponds to a superpixel. Every graph has\n",
    "    edge weights which correspond to concatinations of pixel intensity values. \n",
    "    Each node is connected with at-most 8 neighboors. \n",
    "    \n",
    "    :param image: Image of type 'np.array' having all 3 channels (RGB format).\n",
    "    :param superpixels: The superpixel regions of type 'np.array'.\n",
    "    :return image_graphs: A `list` with graphs for each superpixel. \n",
    "    \"\"\"\n",
    "    image_gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    # Calculate Otsu's threshold\n",
    "    threshold, _ = cv2.threshold(image_gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    num_of_superpixels = len(np.unique(superpixels)) # The number of superpixels-graphs \n",
    "    image_graphs = []\n",
    "    for graph_num in range(1,num_of_superpixels+1):\n",
    "        indices = np.argwhere(superpixels == graph_num)  # Indices of the pixels which are in the specific superpixel\n",
    "        adj_matrix = np.zeros((len(indices), len(indices)))  # Adjacency matrix\n",
    "        node_attributes = {}  # Dictionary with attributes for each vertex.\n",
    "        edges = {}\n",
    "        edge_labels = {}\n",
    "        examined_vertices = []\n",
    "        vertex_index = 0\n",
    "        for vector in indices:\n",
    "            x_coordinate       = vector[0]\n",
    "            y_coordinate       = vector[1]\n",
    "            redChannel_value   = image[x_coordinate,y_coordinate,0] # Red channel pixel value \n",
    "            greenChannel_value = image[x_coordinate,y_coordinate,1] # Green channel pixel value\n",
    "            blueChannel_value  = image[x_coordinate,y_coordinate,2] # Blue channel pixel value\n",
    "                        \n",
    "            # Update dictionary with attributes for each vertex\n",
    "            node_attributes[vertex_index] = [redChannel_value, greenChannel_value, blueChannel_value]\n",
    "            if examined_vertices == []:  # This is the first vertex to be added\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "            else:\n",
    "                # Update adjacency matrix\n",
    "                for index, existing_vertex in enumerate(examined_vertices):\n",
    "                    if Euclidean_distance(existing_vertex, vector) < 1.45:  # Use a 8-neighbourhood\n",
    "                        RGB_1 = node_attributes[index]\n",
    "                        RGB_2 = node_attributes[vertex_index]\n",
    "                        adj_matrix[index, vertex_index] = 1\n",
    "                        adj_matrix[vertex_index, index] = 1\n",
    "                        edges[(index, vertex_index)] = 1\n",
    "                        edges[(vertex_index, index)] = 1\n",
    "                        edge_labels[(index, vertex_index)] = edge_labels_calc(RGB_1, RGB_2)\n",
    "                        edge_labels[(vertex_index, index)] = edge_labels_calc(RGB_1, RGB_2)\n",
    "                examined_vertices.append(vector)\n",
    "                vertex_index = vertex_index + 1\n",
    "        G = Graph(edges, edge_labels=edge_labels)\n",
    "        image_graphs.append(G)\n",
    "    return image_graphs\n",
    "\n",
    "\n",
    "def convert_seg_to_boundaries(seg:np.array) ->np.array:\n",
    "    \"\"\" Convert segmentation which uses a different number for each segment into boundries which is \n",
    "        a numpy array that contains 1 for boundry pixels. \n",
    "    \n",
    "    :param seg: The segmentation in region format of type 'np.array'.\n",
    "    :return result: The segmentation in boundry format of type `np.array`.\n",
    "    \"\"\"\n",
    "    \n",
    "    seg_padded = np.pad(seg, ((1, 0), (1, 0)), mode='constant', constant_values=seg[-1, -1])\n",
    "    dx = cv2.Sobel(seg_padded, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    dy = cv2.Sobel(seg_padded, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    boundaries = np.abs(dx) + np.abs(dy)\n",
    "    boundaries = boundaries[:-1, :-1]\n",
    "    boundaries = cv2.threshold(boundaries, 0, 1, cv2.THRESH_BINARY)[1]\n",
    "    result = boundaries.astype(np.uint8)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate(Predictions: np.array, Human: np.array)->float:\n",
    "    \"\"\" Calculate the F1-score between the predicted boundries and the ground truth. \n",
    "    \n",
    "    :param Predictions: The predicted boundries of type `np.array`.\n",
    "    :param Human: The human labelled boundries of type `np.array`.\n",
    "    :return f1score: F1 score which takes values between 0 and 1. Value is rounded to 4 decimal points and the \n",
    "    closer the value to 1 the better.\n",
    "    \"\"\"\n",
    "    \n",
    "    TP = Predictions * Human\n",
    "    numFP = 0\n",
    "    numFN = 0\n",
    "    nrow, ncol = Predictions.shape\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if (Predictions[i,j] == 1) & (Human[i,j]==0):\n",
    "                numFP = numFP + 1\n",
    "            if (Predictions[i,j] == 0) & (Human[i,j]==1): \n",
    "                numFN = numFN + 1\n",
    "    numTP = np.sum(TP)\n",
    "    f1score = 2 * numTP / (2 * numTP + numFP + numFN)\n",
    "\n",
    "    return round(f1score, 4)\n",
    "\n",
    "def load_all_ground_truth_images()->dict:\n",
    "    \"\"\" Create a dictionary with groundtruth segmentations for each image in the training set.\n",
    "    \n",
    "    :return groundtruth: Dictionary of the form key:[segmentation1,segmentation2,...,segmentationk]. For each image\n",
    "    we might have more than 1 ground truth segmentations.\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_path = \"C:/Users/giwrg/Desktop/Master/Modules/Thesis/Reading Material & Data/2- Data/Data Used - 500/groundTruth/train\"  \n",
    "    # Construct the pattern to match image files\n",
    "    image_pattern = os.path.join(folder_path, '*.mat')  \n",
    "    # Use glob to find all matching image file paths\n",
    "    image_paths = glob.glob(image_pattern)\n",
    "    train_paths = []\n",
    "    groundtruth = {}\n",
    "    # Replace \\ with /\n",
    "    for path in image_paths:\n",
    "        updated_string = path.rsplit(\"\\\\\", 1)\n",
    "        updated_string = \"/\".join(updated_string)\n",
    "        train_paths.append(updated_string)\n",
    "\n",
    "    for path in image_paths:\n",
    "        mat_path = path\n",
    "        # Get the file name from the path\n",
    "        file_name = os.path.basename(path)\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        # Extract the number\n",
    "        image_number = file_name.split(\"\\\\\")[-1]\n",
    "        mat_contents = scipy.io.loadmat(mat_path)\n",
    "        num_of_groundtruths = mat_contents['groundTruth'][0].shape[0]\n",
    "        ground_truths = []\n",
    "        for i in range(0,num_of_groundtruths):\n",
    "            selected_truth = mat_contents['groundTruth'][0][i][0][0][0]\n",
    "            # Downsample true segmentation\n",
    "            selected_truth = downsample_image(selected_truth)\n",
    "            ground_truths.append(selected_truth)\n",
    "        groundtruth[image_number] = ground_truths\n",
    "    return groundtruth\n",
    "\n",
    "\n",
    "def original_Images()->dict:\n",
    "    \n",
    "    \"\"\" Create a dictionary of the form key:value where key = image code and value = image.\n",
    "    \n",
    "    :return original_images: A dictionary of type `dict` of the form image_number:original_image.\n",
    "    \"\"\"\n",
    "    \n",
    "    folder_path_train = \"C:/Users/giwrg/Desktop/Master/Modules/Thesis/Reading Material & Data/2- Data/Data Used - 500/images/train\"  \n",
    "    # Construct the pattern to match image files\n",
    "    image_pattern = os.path.join(folder_path_train, '*.jpg')  \n",
    "    # Use glob to find all matching image file paths\n",
    "    image_paths = glob.glob(image_pattern)\n",
    "    train_paths = []\n",
    "\n",
    "    # Print the paths for each image\n",
    "    for path in image_paths:\n",
    "        updated_string = path.rsplit(\"\\\\\", 1)\n",
    "        updated_string = \"/\".join(updated_string)\n",
    "        train_paths.append(updated_string)\n",
    "\n",
    "    original_images = {}\n",
    "    for path in train_paths:  \n",
    "        mat_path = path\n",
    "        # Get the file name from the path\n",
    "        file_name = os.path.basename(path)\n",
    "        # Remove the file extension\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        # Extract the number\n",
    "        image_number = file_name.split(\"\\\\\")[-1]\n",
    "        current_image = load_image(path)\n",
    "        original_images[image_number] = current_image\n",
    "    return original_images\n",
    "\n",
    "\n",
    "def calc_score(predicted:dict, truth:dict)->list:\n",
    "    \"\"\" Measure performance using F1 score between the predicted segmentation and the truth.\n",
    "    \n",
    "    \n",
    "    :param predicted: The predicted segmentations of type `dict` of the form key:value where\n",
    "    key is the image code and value is the predicted segmentation.\n",
    "    :param truth: The true segmentations of type `dict` of the form key:list[segmentations] where\n",
    "    key is the image code and list[segmentations] is a list of all the human generated segmentations.\n",
    "    :return scores: For each image return the maximum F1 score in a `dict` format. The dictionary stores the \n",
    "    information in the form image_number: max_F1_score.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    image_codes = list(truth.keys())\n",
    "    for code in image_codes:\n",
    "        truths = truth[code]  # List of all the human segmentations\n",
    "        prediction = predicted[code]\n",
    "        max_score = 0\n",
    "        for human_segmentations in truths:\n",
    "            # Convert segmentations into boundry formats\n",
    "            ground_truth_boundry = convert_seg_to_boundaries(human_segmentations)\n",
    "            image_segmentation_boundry = convert_seg_to_boundaries(prediction)\n",
    "\n",
    "            # Calculate performance measures \n",
    "            score = evaluate(image_segmentation_boundry, ground_truth_boundry)        \n",
    "    \n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "            else:\n",
    "                pass\n",
    "        scores[code] = max_score\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b4214e",
   "metadata": {},
   "source": [
    "# Perform HyperParameter Tuning -> Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03273039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the value of the thresholds which have the highest average F1 score among all images in the train set\n",
    "# For each image we are going to use the human segmentation with the MINIMUM number of segments\n",
    "# Load the original images \n",
    "original_Images_dict = original_Images()\n",
    "# Load the ground truths for the original images keeping only the ground truth for each image with MIN number of segments. \n",
    "chosen_truths = load_ground_truth_images()\n",
    "\n",
    "# Set the search space for the hyperparameters of the Canny Edge detector.\n",
    "threshold_values = list(range(50,1050,50))\n",
    "max_avg_score = 0\n",
    "chosen_threshold1 = 0\n",
    "chosen_threshold2 = 0\n",
    "for i in threshold_values:\n",
    "    for j in range(i,1050,50):\n",
    "        current_F1_scores = []\n",
    "        print('Threshold value1: ', i)\n",
    "        print('Threshold value2: ', j)\n",
    "\n",
    "        for image_code in list(original_Images_dict.keys()):\n",
    "            # Get the original image \n",
    "            current_image = original_Images_dict[image_code]\n",
    "            # Apply Canny Edge detector for the current image\n",
    "            edges = cv2.Canny(current_image,i,j)\n",
    "            # Get the chosen truth \n",
    "            truth = chosen_truths[image_code][0]\n",
    "            # Calculate F1 score\n",
    "            # Convert segmentations into boundry formats\n",
    "            ground_truth_boundry = convert_seg_to_boundaries(truth)\n",
    "            image_segmentation_boundry = convert_seg_to_boundaries(edges)\n",
    "\n",
    "            # Calculate performance measures \n",
    "            score = evaluate(image_segmentation_boundry, ground_truth_boundry) \n",
    "            current_F1_scores.append(score)\n",
    "        avg_F1_score = np.mean(current_F1_scores)\n",
    "        print(avg_F1_score)\n",
    "        print()\n",
    "        if avg_F1_score > max_avg_score:\n",
    "            chosen_threshold1 = i\n",
    "            chosen_threshold2 = j\n",
    "            max_avg_score = avg_F1_score\n",
    "\n",
    "print('Chosen threshold1 is: '  , chosen_threshold1)\n",
    "print('Chosen threshold2 is: '  , chosen_threshold2)\n",
    "print('Chosen avg F1 score is: ', max_avg_score )\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4cd48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
